<!doctype html>
<html>
  <head>
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-106761096-1"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());

        gtag('config', 'UA-106761096-1');
    </script>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <title>Tyler LaBonte</title>

    <link rel="stylesheet" href="stylesheets/styles.css">
    <meta name="viewport" content="width=device-width">
  </head>
  <body>
    <div class="wrapper">
      <header>
        <h1>Tyler LaBonte</h1>
        <img src="me.jpeg">
        <p>PhD Student in Machine Learning<br/>Dept. of Industrial & Systems Engineering<br/>Georgia Institute of Technology<br/>Office: CODA S1249H</p>
        <p><a href="mailto:tlabonte@gatech.edu">Email</a> | <a href="cv.pdf">CV</a> | <a href="resume.pdf">Resume</a><br /><a href="https://scholar.google.com/citations?user=0_bKeg4AAAAJ">Scholar</a> | <a href="https://linkedin.com/in/tmlabonte">LinkedIn</a> | <a href="https://github.com/tmlabonte">GitHub</a> | <a href="https://twitter.com/tmlabonte">Twitter</a></p>
        <p><a href="https://drive.google.com/drive/folders/1Cw1rIdJA9FTcsJS12zSvjB4uwj_Gwck5?usp=sharing">Fellowship Materials</a><br/><a href="exposition.html">Favorite Expository Articles</a></p>
        <p><b>Georgia Tech undergrads:</b> Please contact me! I am happy to chat about research (or anything, really).
      </header>
      <section>
        <h2>About Me</h2>
        <p>I am a third-year PhD student in Machine Learning at the Georgia Institute of Technology advised by <a href="https://vmuthukumar.ece.gatech.edu/">Vidya Muthukumar</a> and <a href="https://faculty.cc.gatech.edu/~jabernethy9/">Jacob Abernethy</a>. I completed my BS in Applied and Computational Mathematics at the University of Southern California advised by <a href="https://viterbi-web.usc.edu/~shaddin/">Shaddin Dughmi</a>, where I was a Trustee Scholar and Viterbi Fellow. My work is generously supported by the DoD NDSEG Fellowship.</p>

        <p>I am interested in advancing our scientific understanding of deep learning using both theory and experimentation. My current focus is characterizing the generalization phenomena of overparameterized neural networks and developing provable algorithms for efficient, accurate, and robust learning. I also work on applying mathematically-justified techniques to large-scale language and vision models. The ultimate goal of my research is to enable the safe and trusted deployment of deep learning systems in high-consequence applications such as medicine, defense, and energy.</p>

        <hr style="margin:12px;">
        <h3 style="margin-bottom:10px;">Recent News</h3>
        <ul style="margin-bottom:10px;">
            <li><b>September 2023:</b> Collaboration with Google DeepMind on <a href="https://arxiv.org/abs/2309.08534">Towards Last-layer Retraining for Group Robustness without Group Annotations</a> was accepted to NeurIPS 2023.
            <li><b>July 2023:</b> Started research internship at Google! I'll be working on LLMs for hardware-software code design.
            <li><b>June 2023:</b> Collaboration with Google DeepMind on <a href="https://openreview.net/pdf?id=IGPde2wLse">Saving a Split for Last-layer Retraining can Improve Group Robustness without Group Annotations</a> was accepted to the ICML 2023 Workshop on Spurious Correlations, Invariance, and Stability.
            <li><b>December 2022:</b> Passed the ML PhD qualifying exam!
            <li><b>October 2022:</b> Collaboration with Google Research on <a href="https://openreview.net/pdf?id=3OxII8ZB3A">Dropout Disagreement: A Recipe for Group Robustness with Fewer Annotations</a> was accepted to the NeurIPS 2022 Workshop on Distribution Shifts.</li>
            <li><b>October 2022:</b> Collaboration with Microsoft Research and Meta AI on <a href="https://openaccess.thecvf.com/content/WACV2023/papers/LaBonte_Scaling_Novel_Object_Detection_With_Weakly_Supervised_Detection_Transformers_WACV_2023_paper.pdf">Scaling Novel Object Detection with Weakly Supervised Detection Transformers</a> was accepted to WACV 2023.</li>
            <li><b>June 2022:</b> Received $2,000 travel grant to attend the Simons Institute <a href="https://simons.berkeley.edu/workshops/deep-learning-theory-workshop">Deep Learning Theory Workshop</a>.</li>
            <!--<li><b>May 2022:</b> Collaboration with Microsoft Research on <a href="https://arxiv.org/abs/2207.05205">Scaling Novel Object Detection with Weakly Supervised Detection Transformers</a> was accepted to the CVPR 2022 Workshop on Transformers for Vision.</li>-->
        </ul>  
        <hr>

        <h2>Publications</h2>
        <!---<h3>Preprints</h3>--->
        <h3>Conference Articles</h3>
        <ol>
            <li>
                <a href="https://arxiv.org/abs/2309.08534">Towards Last-layer Retraining for Group Robustness with Fewer Annotations.</a><br>
                Tyler LaBonte, Vidya Muthukumar, and Abhishek Kumar.<br>
                <b>NeurIPS 2023.</b>
            </li>
            <li>
                <a href="https://openaccess.thecvf.com/content/WACV2023/papers/LaBonte_Scaling_Novel_Object_Detection_With_Weakly_Supervised_Detection_Transformers_WACV_2023_paper.pdf">Scaling Novel Object Detection with Weakly Supervised Detection Transformers.</a><br>
                Tyler LaBonte, Yale Song, Xin Wang, Vibhav Vineet, and Neel Joshi.<br>
                <b>WACV 2023.</b> <a href="labonte23wacv_poster.pdf">Poster</a>
            </li>
        </ol>
        <h3>Journal Articles</h3>
        <ol>
            <li>
                <a href="https://www.ics.uci.edu/~mikes/papers/Student_Misconceptions_Dynamic_Programming.pdf">Student Misconceptions of Dynamic Programming: A Replication Study.</a><br>
                Michael Shindler, Natalia Pinpin, Mia Markovic, Frederick Reiber, Jee Hoon Kim, Giles Pierre Nunez Carlos, Mine Dogucu, Mark Hong, Michael Luu, Brian Anderson, Aaron Cote, Matthew Ferland, Palak Jain, Tyler LaBonte, Leena Mathur, Ryan Moreno, and Ryan Sakuma.<br>
                <b>Computer Science Education</b>, 32(3):288&mdash;312, 2022.
            </li>
            <li>
                <a href="https://doi.org/10.1038/s41467-021-25493-8">Quantifying the Unknown Impact of Segmentation Uncertainty on Image-Based Simulations.</a><br>
                Michael C. Krygier, Tyler LaBonte, Carianne Martinez, Chance Norris, Krish Sharma, Lincoln N. Collins, Partha P. Mukherjee, and Scott A. Roberts.<br>
                <b>Nature Communications</b>, 12(1):5414, 2021.
            </li>
        </ol>
        <h3>Workshop Articles</h3>
        <ol>
            <li>
                <a href="https://openreview.net/pdf?id=IGPde2wLse">Saving a Split for Last-layer Retraining can Improve Group Robustness without Group Annotations.</a><br>
                Tyler LaBonte, Vidya Muthukumar, and Abhishek Kumar.<br>
                <b>ICML 2023</b> Workshop on Spurious Correlations, Invariance, and Stability. <a href="labonte23icml_poster.pdf">Poster</a>
            </li>
            <li>
                <a href="https://openreview.net/pdf?id=3OxII8ZB3A">Dropout Disagreement: A Recipe for Group Robustness with Fewer Annotations.</a><br>
                Tyler LaBonte, Vidya Muthukumar, and Abhishek Kumar.<br>
                <b>NeurIPS 2022</b> Workshop on Distribution Shifts. <a href="labonte22neurips_poster.pdf">Poster</a>
            </li>
            <li>
                <a href="https://arxiv.org/abs/2207.05205">Scaling Novel Object Detection with Weakly Supervised Detection Transformers.</a><br>
                Tyler LaBonte, Yale Song, Xin Wang, Vibhav Vineet, and Neel Joshi.<br>
                <b>CVPR 2022</b> Workshop on Transformers for Vision. <a href="labonte22cvpr_poster.pdf">Poster</a>
            </li>
        </ol>
        <h3>Theses</h3>
        <ol>
            <li>
                <a href="thesis.pdf">Finding the Needle in a High-Dimensional Haystack: Oracle Methods for Convex Optimization.</a><br>
                Tyler LaBonte.<br>
                Undergraduate Thesis, University of Southern California, 2021.<br>
                Winner of the USC Discovery Scholar distinction.
            </li>
        </ol>
        <h3>Manuscripts</h3>
        <ol>
            <li>
                <a href="https://arxiv.org/abs/1910.10793">We Know Where We Don't Know: 3D Bayesian CNNs for Credible Geometric Uncertainty.</a><br>
                Tyler LaBonte, Carianne Martinez, and Scott A. Roberts.<br>
                Manuscript, 2019.
            </li>
        </ol>

        <h2>Selected Awards</h2>
        <ol>
          <li>Simons Institute Deep Learning Theory Workshop Travel Grant ($2,000)</li>
          <li>DoD NDSEG Fellowship ($170,000)</li>
          <li>NSF Graduate Research Fellowship ($138,000&mdash;declined)</li>
          <li>USC Discovery Scholar (research distinction for &lt;100 USC graduates)</li>
          <li>USC Trustee Scholar (Full scholarship worth $225,000)</li>
          <li>USC Viterbi Fellow (Research funding worth $24,000)</li>
        </ol>

        <h2>Industry Research Experience</h2>
        <ol>
          <li>Machine Learning Research Intern, Google (2023)</li>
          <li>Machine Learning Research Intern, Microsoft Research (2021&mdash;2022)</li>
          <li>Machine Learning Research Intern, Google X (2020)</li>
          <li>Machine Learning Research Intern, Sandia National Labs (2019&mdash;2020)</li>
        </ol>

        <h2>Advising</h2>
        <ol>
            <li> John C. Hill&mdash;Georgia Tech MS (2022&mdash;)</li>
            <li> <a href="https://www.linkedin.com/in/pratik-deolasi-71a679194/">Pratik Deolasi</a>&mdash;Georgia Tech BS &rarr; MathWorks (2021&mdash;2022)</li>
            <li> <a href="https://www.linkedin.com/in/rmahuja/">Rishit Mohan Ahuja</a>&mdash;Georgia Tech BS &rarr; Georgia Tech MS (2021&mdash;2022)</li>
        </ol>

        <h2>Teaching</h2>
        <ol>
            <li><a href="https://mltheory.github.io/CS7545/">Georgia Tech CS 7545: Machine Learning Theory (2023)</a></li>
            <li>USC CSCI 270: Intro to Algorithms and Theory of Computing (2021)</li>
            <li><a href="http://caisplusplus.usc.edu/">USC Center for AI in Society:</a> Introduction to Machine Learning (2020)</li>
            <li>USC CSCI 170: Discrete Methods in Computer Science (2019)</li>
        </ol>

        <h2>Reviewing</h2>
        <ol>
            <li>Reviewer, ICLR 2024</li>
            <li>Reviewer, NeurIPS 2023</li>
        </ol>

        <h2>Service and Leadership</h2>
        <ol>
            <li>System Administrator, Georgia Tech ML Theory GPU Cluster (2022&mdash;)</li>
            <li>Organizer, <a href="https://mltheory.github.io/">Georgia Tech ML Theory Reading Group</a> (2021&mdash;2023)</li>
            <li>Projects Lead, <a href="http://caisplusplus.usc.edu/">USC Center for AI in Society</a> (2019)</li>
            <li>Associate Director of Robotics, <a href="https://viterbik12.usc.edu/">USC Viterbi K-12 STEM Center</a> (2018)</li>
            <li>Robotics Mentor, <a href="https://viterbik12.usc.edu/">USC Viterbi K-12 STEM Center</a> (2017&mdash;2018)</li>
        </ol>

        <h2>Other Activities</h2>
        <ol>
            <li>Fleet Captain, <a href="https://www.gtsailing.org/">Georgia Tech Sailing Club</a> (2023&mdash;)</li>
            <li>House Chair, <a href="https://uschawaiiclub.weebly.com/">USC Hawai'i Club</a> (2020&mdash;2021)</li>
            <li>Vice President of Finance, <a href="https://uschawaiiclub.weebly.com/">USC Hawai'i Club</a> (2019&mdash;2020)</li>
        <ol>
     </section>

      <footer>
        <p><small>Theme by <a href="https://github.com/orderedlist">orderedlist</a></small></p>
      </footer>
    </div>
    <script src="javascripts/scale.fix.js"></script>
  </body>
</html>
