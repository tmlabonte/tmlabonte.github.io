<!doctype html>
<html>
  <head>
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-106761096-1"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());

        gtag('config', 'UA-106761096-1');
    </script>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <title>Tyler LaBonte</title>

    <link rel="stylesheet" href="stylesheets/styles.css">
    <meta name="viewport" content="width=device-width">
  </head>
  <body>
    <div class="wrapper">
      <header>
        <h1>Tyler LaBonte</h1>
        <img src="me.png">
        <p>PhD Student in Machine Learning<br/>School of Industrial/Systems Engineering<br/>Georgia Institute of Technology<br/>Office: CODA S1249H</p>
        <p><a href="mailto:tlabonte@gatech.edu">Email</a> | <a href="cv.pdf">CV</a> | <a href="resume.pdf">Resume</a><br /><a href="https://scholar.google.com/citations?user=0_bKeg4AAAAJ">Scholar</a> | <a href="https://linkedin.com/in/tmlabonte">LinkedIn</a> | <a href="https://github.com/tmlabonte">GitHub</a> | <a href="https://twitter.com/tmlabonte">Twitter</a></p>
        <p><a href="https://drive.google.com/drive/folders/1Cw1rIdJA9FTcsJS12zSvjB4uwj_Gwck5?usp=sharing">Fellowship Materials</a><br/><a href="exposition.html">Favorite Expository Articles</a></p>
        <p><b>Georgia Tech undergrads:</b> Please contact me! I am happy to chat about research (or anything, really).
      </header>
      <section>
        <h2>About Me</h2>
        <p>I am a fourth-year PhD student in Machine Learning at the Georgia Institute of Technology advised by <a href="https://vmuthukumar.ece.gatech.edu/">Vidya Muthukumar</a> and <a href="https://faculty.cc.gatech.edu/~jabernethy9/">Jacob Abernethy</a>. I completed my BS in Applied and Computational Mathematics at the University of Southern California advised by <a href="https://viterbi-web.usc.edu/~shaddin/">Shaddin Dughmi</a>, where I was a Trustee Scholar and Viterbi Fellow. My work has been generously supported by the DoD NDSEG Fellowship.</p>

        <p>I am interested in advancing our scientific understanding of deep learning and using theoretical insights to design methods which work well in practice. My current focus is characterizing the generalization phenomena of overparameterized neural networks and developing provable algorithms for robust learning, particularly under distribution shift. The ultimate goal of my research is to enable the safe and trusted deployment of deep learning systems in high-consequence applications such as medicine, defense, and energy.</p>

        <p>My industry research experience has focused on capabilities of large-scale language and vision models. During my PhD, I interned at Google where I leveraged the Gemini LLM for hardware-software code design and at Microsoft Research where I developed Detection Transformers for weakly supervised object detection.</p>

        <!--<hr style="margin:12px;">-->
        <!--<h3 style="margin-bottom:10px;">Recent News</h3>-->
        <!--<ul style="margin-bottom:10px;">-->
            <!---<li><b>July 2024:</b> New preprint on <a href="https://arxiv.org/abs/2407.13957">The Group Robustness is in the Details: Revisiting Finetuning under Spurious Correlations</a>.
            <li><b>December 2023:</b> Presented our collaboration with Google DeepMind on <a href="https://arxiv.org/abs/2309.08534">Towards Last-layer Retraining for Group Robustness with Fewer Annotations</a> at NeurIPS 2023 in New Orleans.
            <li><b>October 2023:</b> Enjoyed helping organize the annual <a href="https://www.let-all.com/fall23.html">Learning Theory Alliance Workshop</a>. Thanks to all the co-organizers, speakers, volunteers, and participants!
            <li><b>September 2023:</b> Collaboration with Google DeepMind on <a href="https://arxiv.org/abs/2309.08534">Towards Last-layer Retraining for Group Robustness with Fewer Annotations</a> was accepted to NeurIPS 2023.
            <li><b>July 2023:</b> Started research internship at Google! I'll be working on LLMs for hardware-software code design.
            <li><b>June 2023:</b> Collaboration with Google DeepMind on <a href="https://openreview.net/pdf?id=IGPde2wLse">Saving a Split for Last-layer Retraining can Improve Group Robustness without Group Annotations</a> was accepted to the ICML 2023 Workshop on Spurious Correlations, Invariance, and Stability.
            <li><b>December 2022:</b> Passed the ML PhD qualifying exam!
            <li><b>October 2022:</b> Collaboration with Google Research on <a href="https://openreview.net/pdf?id=3OxII8ZB3A">Dropout Disagreement: A Recipe for Group Robustness with Fewer Annotations</a> was accepted to the NeurIPS 2022 Workshop on Distribution Shifts.</li>
            <li><b>October 2022:</b> Collaboration with Microsoft Research and Meta AI on <a href="https://openaccess.thecvf.com/content/WACV2023/papers/LaBonte_Scaling_Novel_Object_Detection_With_Weakly_Supervised_Detection_Transformers_WACV_2023_paper.pdf">Scaling Novel Object Detection with Weakly Supervised Detection Transformers</a> was accepted to WACV 2023.</li>
            <li><b>June 2022:</b> Received $2,000 travel grant to attend the Simons Institute <a href="https://simons.berkeley.edu/workshops/deep-learning-theory-workshop">Deep Learning Theory Workshop</a>.</li>
            <li><b>May 2022:</b> Collaboration with Microsoft Research on <a href="https://arxiv.org/abs/2207.05205">Scaling Novel Object Detection with Weakly Supervised Detection Transformers</a> was accepted to the CVPR 2022 Workshop on Transformers for Vision.</li>-->
        </ul>
        <hr>

        <h2>Publications</h2>
        An asterisk (*) denotes equal contribution.<br><br>
        <h3>Preprints</h3>
        <ol>
            <li>
                Task Shift: From Classification to Regression in Overparameterized Linear Models.<br>
                Tyler LaBonte*, Kuo-Wei Lai*, and Vidya Muthukumar.<br>
                Under submission.
            </li>
        </ol>
        <h3>Conference Articles</h3>
        <ol>
            <li>
                <a href="https://arxiv.org/abs/2407.13957">The Group Robustness is in the Details: Revisiting Finetuning under Spurious Correlations.</a><br>
                Tyler LaBonte, John C. Hill, Xinchen Zhang, Vidya Muthukumar, and Abhishek Kumar.<br>
                <b>NeurIPS 2024.</b> [<a href="https://arxiv.org/abs/2407.13957">arxiv</a>] [<a href="https://github.com/tmlabonte/revisiting-finetuning">code</a>] [<a href="posters/labonte24neurips_poster.pdf">poster</a>]
            </li>

            <li>
                <a href="https://proceedings.neurips.cc/paper_files/paper/2023/file/265bee74aee86df77e8e36d25e786ab5-Paper-Conference.pdf">Towards Last-layer Retraining for Group Robustness with Fewer Annotations.</a><br>
                Tyler LaBonte, Vidya Muthukumar, and Abhishek Kumar.<br>
                <b>NeurIPS 2023.</b> [<a href="https://arxiv.org/abs/2309.08534">arxiv</a>] [<a href="https://github.com/tmlabonte/last-layer-retraining">code</a>] [<a href="posters/labonte23neurips_poster.pdf">poster</a>] [<a href="https://www.youtube.com/watch?v=7VHsXUckerc">video</a>]
            </li>
            <li>
                <a href="https://openaccess.thecvf.com/content/WACV2023/papers/LaBonte_Scaling_Novel_Object_Detection_With_Weakly_Supervised_Detection_Transformers_WACV_2023_paper.pdf">Scaling Novel Object Detection with Weakly Supervised Detection Transformers.</a><br>
                Tyler LaBonte, Yale Song, Xin Wang, Vibhav Vineet, and Neel Joshi.<br>
                <b>WACV 2023.</b> [<a href="https://arxiv.org/abs/2207.05205">arxiv</a>] [<a href="https://github.com/tmlabonte/weakly-supervised-DETR">code</a>] [<a href="posters/labonte23wacv_poster.pdf">poster</a>]
            </li>
        </ol>
        <h3>Journal Articles</h3>
        <ol>
            <li>
                <a href="https://www.ics.uci.edu/~mikes/papers/Student_Misconceptions_Dynamic_Programming.pdf">Student Misconceptions of Dynamic Programming: A Replication Study.</a><br>
                Michael Shindler, Natalia Pinpin, Mia Markovic, Frederick Reiber, Jee Hoon Kim, Giles Pierre Nunez Carlos, Mine Dogucu, Mark Hong, Michael Luu, Brian Anderson, Aaron Cote, Matthew Ferland, Palak Jain, Tyler LaBonte, Leena Mathur, Ryan Moreno, and Ryan Sakuma.<br>
                <b>Computer Science Education</b>, 32(3):288&mdash;312, 2022.
            </li>
            <li>
                <a href="https://doi.org/10.1038/s41467-021-25493-8">Quantifying the Unknown Impact of Segmentation Uncertainty on Image-Based Simulations.</a><br>
                Michael C. Krygier, Tyler LaBonte, Carianne Martinez, Chance Norris, Krish Sharma, Lincoln N. Collins, Partha P. Mukherjee, and Scott A. Roberts.<br>
                <b>Nature Communications</b>, 12(1):5414, 2021. [<a href="https://data.mendeley.com/datasets/g3hr4rkb48/3">code</a>]
            </li>
        </ol>
        <h3>Workshop Articles</h3>
        <ol>
            <li>
                <a href="https://openreview.net/pdf?id=IGPde2wLse">Saving a Split for Last-layer Retraining can Improve Group Robustness without Group Annotations.</a><br>
                Tyler LaBonte, Vidya Muthukumar, and Abhishek Kumar.<br>
                <b>ICML 2023</b> Workshop on Spurious Correlations, Invariance, and Stability. [<a href="https://github.com/tmlabonte/last-layer-retraining">code</a>] [<a href="posters/labonte23icml_poster.pdf">poster</a>]
            </li>
            <li>
                <a href="https://openreview.net/pdf?id=3OxII8ZB3A">Dropout Disagreement: A Recipe for Group Robustness with Fewer Annotations.</a><br>
                Tyler LaBonte, Vidya Muthukumar, and Abhishek Kumar.<br>
                <b>NeurIPS 2022</b> Workshop on Distribution Shifts. [<a href="https://github.com/tmlabonte/last-layer-retraining">code</a>] [<a href="posters/labonte22neurips_poster.pdf">poster</a>]
            </li>
            <li>
                <a href="labonte22cvpr.pdf">Scaling Novel Object Detection with Weakly Supervised Detection Transformers.</a><br>
                Tyler LaBonte, Yale Song, Xin Wang, Vibhav Vineet, and Neel Joshi.<br>
                <b>CVPR 2022</b> Workshop on Transformers for Vision. [<a href="https://github.com/tmlabonte/weakly-supervised-DETR">code</a>] [<a href="posters/labonte22cvpr_poster.pdf">poster</a>]
            </li>
        </ol>
        <h3>Theses</h3>
        <ol>
            <li>
                <a href="thesis.pdf">Finding the Needle in a High-Dimensional Haystack: Oracle Methods for Convex Optimization.</a><br>
                Tyler LaBonte.<br>
                Undergraduate Thesis, University of Southern California, 2021.<br>
                Winner of the USC Discovery Scholar distinction.
            </li>
        </ol>
        <h3>Manuscripts</h3>
        <ol>
            <li>
                <a href="https://arxiv.org/abs/1910.10793">We Know Where We Don't Know: 3D Bayesian CNNs for Credible Geometric Uncertainty.</a><br>
                Tyler LaBonte, Carianne Martinez, and Scott A. Roberts.<br>
                Manuscript, 2019. [<a href="https://github.com/sandialabs/bcnn">code</a>]
            </li>
        </ol>

        <h2>Selected Awards</h2>
        <ol>
          <li>Simons Institute Deep Learning Theory Workshop Travel Grant ($2,000)</li>
          <li>DoD NDSEG Fellowship ($170,000)</li>
          <li>NSF Graduate Research Fellowship ($138,000&mdash;declined)</li>
          <li>USC Discovery Scholar (research distinction for &lt;100 USC graduates)</li>
          <li>USC Trustee Scholar (Full scholarship worth $225,000)</li>
          <li>USC Viterbi Fellow (Research funding worth $24,000)</li>
        </ol>

        <h2>Industry Research Experience</h2>
        <ol>
          <li>Machine Learning Research Intern, Google (2023)</li>
          <li>Machine Learning Research Intern, Microsoft Research (2021&mdash;2022)</li>
          <li>Machine Learning Research Intern, Google X (2020)</li>
          <li>Machine Learning Research Intern, Sandia National Labs (2019&mdash;2020)</li>
        </ol>

        <h2>Advising</h2>
        <ol>
            <li> Xinchen Zhang&mdash;Georgia Tech MS (2024&mdash;)</li>
            <li> John C. Hill&mdash;Georgia Tech BS/MS &rarr; Georgia Tech PhD (2022&mdash;2024)</li>
        </ol>

        <h2>Teaching</h2>
        <ol>
            <li>Lecturer/TA (8 lectures), <a href="https://mltheory.github.io/CS7545/">Georgia Tech CS 7545: Machine Learning Theory (2024)</a></li>
            <li>Lecturer/TA (12 lectures), <a href="https://mltheory.github.io/CS7545/">Georgia Tech CS 7545: Machine Learning Theory (2023)</a></li>
            <li>Teaching Assistant, USC CSCI 270: Intro to Algorithms and Theory of Computing (2021)</li>
            <li>Instructor, <a href="http://caisplusplus.usc.edu/">USC Center for AI in Society:</a> Introduction to Machine Learning (2020)</li>
            <li>Teaching Assistant, USC CSCI 170: Discrete Methods in Computer Science (2019)</li>
        </ol>

        <h2>Reviewing</h2>
        <ol>
            <li>Reviewer, ICML 2025</li>
            <li>Reviewer, NeurIPS 2024</li>
            <li>Reviewer, ICLR 2024</li>
            <li>Reviewer, NeurIPS 2023</li>
        </ol>

        <h2>Service and Leadership</h2>
        <ol>
            <li>System Administrator, Georgia Tech ML Theory GPU Cluster (2022&mdash;)</li>
            <li>Student Organizer, <a href="https://www.let-all.com/fall23.html">Learning Theory Alliance Workshop</a> (2023)</li>
            <li>Organizer, <a href="https://mltheory.github.io/">Georgia Tech ML Theory Reading Group</a> (2021&mdash;2023)</li>
            <li>Projects Lead, <a href="http://caisplusplus.usc.edu/">USC Center for AI in Society</a> (2019)</li>
            <li>Associate Director of Robotics, <a href="https://viterbik12.usc.edu/">USC Viterbi K-12 STEM Center</a> (2018)</li>
            <li>Robotics Mentor, <a href="https://viterbik12.usc.edu/">USC Viterbi K-12 STEM Center</a> (2017&mdash;2018)</li>
        </ol>

        <h2>Other Activities</h2>
        <ol>
            <li>Fleet Captain, <a href="https://www.gtsailing.org/">Georgia Tech Sailing Club</a> (2023&mdash;)</li>
            <li>House Chair, <a href="https://uschawaiiclub.weebly.com/">USC Hawai'i Club</a> (2020&mdash;2021)</li>
            <li>Vice President of Finance, <a href="https://uschawaiiclub.weebly.com/">USC Hawai'i Club</a> (2019&mdash;2020)</li>
        <ol>
     </section>

      <footer>
        <p><small>Theme by <a href="https://github.com/orderedlist">orderedlist</a></small></p>
      </footer>
    </div>
    <script src="javascripts/scale.fix.js"></script>
  </body>
</html>
